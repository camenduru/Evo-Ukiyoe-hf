import random

from PIL import Image
from diffusers import EulerDiscreteScheduler
import gradio as gr
import numpy as np
import spaces
import torch

# torch._inductor.config.conv_1x1_as_mm = True
# torch._inductor.config.coordinate_descent_tuning = True
# torch._inductor.config.epilogue_fusion = False
# torch._inductor.config.coordinate_descent_check_all_directions = True
from evo_ukiyoe_v1 import load_evo_ukiyoe


DESCRIPTION = """# ğŸŸ Evo-Ukiyoe
ğŸ¤— [ãƒ¢ãƒ‡ãƒ«ä¸€è¦§](https://huggingface.co/SakanaAI) | ğŸ“ [ãƒ–ãƒ­ã‚°](https://sakana.ai/evo-ukiyoe/) | ğŸ¦ [Twitter](https://twitter.com/SakanaAILabs)

[Evo-Ukiyoe](https://huggingface.co/SakanaAI/Evo-Ukiyoe-v1)ã¯[Sakana AI](https://sakana.ai/)ãŒæ•™è‚²ç›®çš„ã§é–‹ç™ºã—ãŸæµ®ä¸–çµµã«ç‰¹åŒ–ã—ãŸç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
å…¥åŠ›ã—ãŸæ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ²¿ã£ãŸæµ®ä¸–çµµé¢¨ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚ˆã‚Šè©³ã—ãã¯ã€ä¸Šè¨˜ã®ãƒ–ãƒ­ã‚°ã‚’ã”å‚ç…§ãã ã•ã„ã€‚
"""
if not torch.cuda.is_available():
    DESCRIPTION += "\n<p>Running on CPU ğŸ¥¶ This demo may not work on CPU.</p>"

MAX_SEED = np.iinfo(np.int32).max

device = "cuda" if torch.cuda.is_available() else "cpu"

NUM_IMAGES_PER_PROMPT = 1
SAFETY_CHECKER = True
if SAFETY_CHECKER:
    from safety_checker import StableDiffusionSafetyChecker
    from transformers import CLIPFeatureExtractor

    safety_checker = StableDiffusionSafetyChecker.from_pretrained(
        "CompVis/stable-diffusion-safety-checker"
    ).to(device)
    feature_extractor = CLIPFeatureExtractor.from_pretrained(
        "openai/clip-vit-base-patch32"
    )

    def check_nsfw_images(
        images: list[Image.Image],
    ) -> tuple[list[Image.Image], list[bool]]:
        safety_checker_input = feature_extractor(images, return_tensors="pt").to(device)
        has_nsfw_concepts = safety_checker(
            images=[images], clip_input=safety_checker_input.pixel_values.to(device)
        )

        return images, has_nsfw_concepts


pipe = load_evo_ukiyoe(device)
pipe.scheduler = EulerDiscreteScheduler.from_config(
    pipe.scheduler.config, use_karras_sigmas=True,
)
pipe.to(device=device, dtype=torch.float16)
# pipe.unet.to(memory_format=torch.channels_last)
# pipe.vae.to(memory_format=torch.channels_last)
# # Compile the UNet and VAE.
# pipe.unet = torch.compile(pipe.unet, mode="max-autotune", fullgraph=True)
# pipe.vae.decode = torch.compile(pipe.vae.decode, mode="max-autotune", fullgraph=True)


def randomize_seed_fn(seed: int, randomize_seed: bool) -> int:
    if randomize_seed:
        seed = random.randint(0, MAX_SEED)
    return seed


@spaces.GPU
@torch.inference_mode()
def generate(
    prompt: str,
    negative_prompt: str = "",
    seed: int = 0,
    randomize_seed: bool = False,
    progress=gr.Progress(track_tqdm=True),
):
    seed = int(randomize_seed_fn(seed, randomize_seed))
    generator = torch.Generator().manual_seed(seed)

    images = pipe(
        prompt=prompt + "æœ€é«˜å“è³ªã®è¼»ã®æµ®ä¸–çµµã€‚è¶…è©³ç´°ã€‚",
        negative_prompt=negative_prompt,
        width=1024,
        height=1024,
        guidance_scale=8.0,
        num_inference_steps=40,
        generator=generator,
        num_images_per_prompt=NUM_IMAGES_PER_PROMPT,
        output_type="pil",
    ).images

    if SAFETY_CHECKER:
        images, has_nsfw_concepts = check_nsfw_images(images)
        if any(has_nsfw_concepts):
            gr.Warning("NSFW content detected.")
            return Image.new("RGB", (512, 512), "WHITE"), seed
    return images[0], seed


examples = [
    "æ¤ç‰©ã¨èŠ±ãŒã‚ã‚Šã¾ã™ã€‚è¶ãŒé£›ã‚“ã§ã„ã¾ã™ã€‚",
    "é¶´ãŒåº­ã«ç«‹ã£ã¦ã„ã¾ã™ã€‚é›ªãŒé™ã£ã¦ã„ã¾ã™ã€‚",
    "ç€ç‰©ã‚’ç€ã¦ã„ã‚‹çŒ«ãŒåº­ã§ãŠèŒ¶ã‚’é£²ã‚“ã§ã„ã¾ã™ã€‚",
]

css = """
.gradio-container{max-width: 690px !important}
h1{text-align:center}
"""
with gr.Blocks(css=css) as demo:
    gr.Markdown(DESCRIPTION)
    with gr.Group():
        with gr.Row():
            prompt = gr.Textbox(placeholder="æ—¥æœ¬èªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", show_label=False, scale=8)
            submit = gr.Button(scale=0)
        result = gr.Image(label="Evo-Ukiyoeã‹ã‚‰ã®ç”Ÿæˆçµæœ", type="pil", show_label=False)
    with gr.Accordion("è©³ç´°è¨­å®š", open=False):
        negative_prompt = gr.Textbox(placeholder="æ—¥æœ¬èªã§ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚(ç©ºç™½å¯)", show_label=False)
        seed = gr.Slider(label="ã‚·ãƒ¼ãƒ‰å€¤", minimum=0, maximum=MAX_SEED, step=1, value=0)
        randomize_seed = gr.Checkbox(label="ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ¼ãƒ‰å€¤ã‚’æ±ºå®š", value=True)
    gr.Examples(examples=examples, inputs=[prompt], outputs=[result, seed], fn=generate)
    gr.on(
        triggers=[
            prompt.submit,
            submit.click,
        ],
        fn=generate,
        inputs=[
            prompt,
            negative_prompt,
            seed,
            randomize_seed,
        ],
        outputs=[result, seed],
        api_name="run",
    )
    gr.Markdown("""âš ï¸ æœ¬ãƒ¢ãƒ‡ãƒ«ã¯å®Ÿé¨“æ®µéšã®ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã§ã‚ã‚Šã€æ•™è‚²ãŠã‚ˆã³ç ”ç©¶é–‹ç™ºã®ç›®çš„ã§ã®ã¿æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚å•†ç”¨åˆ©ç”¨ã‚„ã€éšœå®³ãŒé‡å¤§ãªå½±éŸ¿ã‚’åŠã¼ã™å¯èƒ½æ€§ã®ã‚ã‚‹ç’°å¢ƒï¼ˆãƒŸãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªç’°å¢ƒï¼‰ã§ã®ä½¿ç”¨ã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚
                æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã¯ã€åˆ©ç”¨è€…ã®è‡ªå·±è²¬ä»»ã§è¡Œã‚ã‚Œã€ãã®æ€§èƒ½ã‚„çµæœã«ã¤ã„ã¦ã¯ä½•ã‚‰ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã€‚
                Sakana AIã¯ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸç›´æ¥çš„ã¾ãŸã¯é–“æ¥çš„ãªæå¤±ã«å¯¾ã—ã¦ã€çµæœã«é–¢ã‚ã‚‰ãšã€ä¸€åˆ‡ã®è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
                åˆ©ç”¨è€…ã¯ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã«ä¼´ã†ãƒªã‚¹ã‚¯ã‚’ååˆ†ã«ç†è§£ã—ã€è‡ªèº«ã®åˆ¤æ–­ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™ã€‚""")

demo.queue().launch()
